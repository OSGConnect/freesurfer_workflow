#!/usr/bin/env python
import argparse
import glob
import sys
import os
import getpass
import subprocess
import re
import cStringIO
import time
import shutil
import cPickle
import grp

import fsurfer
import Pegasus.DAX3

VERSION = fsurfer.__version__

WORKFLOW_BASE_DIRECTORY = os.path.join('/stash2/user',
                                       getpass.getuser(),
                                       'freesurfer_scratch',
                                       'freesurfer')
WORKFLOW_OUTPUT_DIRECTORY = os.path.join(WORKFLOW_BASE_DIRECTORY,
                                         'output',
                                         getpass.getuser(),
                                         'pegasus',
                                         'freesurfer')
WORKFLOW_DIRECTORY = os.path.join(WORKFLOW_BASE_DIRECTORY,
                                  getpass.getuser(),
                                  'pegasus',
                                  'freesurfer')
PEGASUSRC_PATH = os.path.expanduser('~/.fsurf/pegasusrc')


def check_and_create_workflow_dir(workflow_dir=WORKFLOW_BASE_DIRECTORY):
    """
    Check for the presence of a directory to hold workflows and
    create one if necessary

    :param workflow_dir: path to directory for workflows
    :return: True if directory exists or has been created, False otherwise
    """
    try:
        if os.path.isdir(workflow_dir):
            return True
        elif os.path.exists(workflow_dir) and not os.path.isdir(workflow_dir):
            sys.stdout.write("AreCan't create {0} because it ".format(workflow_dir) +
                             "is already present and not a directory\n")
            return False
        else:
            sys.stdout.write("Creating directory for workflows\n")
            os.makedirs(workflow_dir, 0o700)
            return True
    except OSError:
        sys.stdout.write("Can't create {0} for workflows, "
                         "exiting...\n".format(workflow_dir))
        return False


def run_pegasus(action, **kwargs):
    """
    Run pegasus to complete the specified action and return the output from
    pegasus, keyword arguments that might be used are:
        workflow_id -- id for pegasus workflow, used for remove and status actions
        dax_location -- path to xml file with DAX, used for submit
        workflow_directory  - directory for pegasus to keep it's workflow
                              information use for submit
        conf -- path to pegasusrc config used for submit


    :param action:      a string specifying the action (remove, status, submit)
    :return:            the output from pegasus
    """
    if not check_and_create_workflow_dir():
        return 1, "Can't create workflow directory"
    if action in ['status', 'remove']:
        if 'workflow_id' not in kwargs:
            return 1, "Workflow id missing"
        workflow_location = os.path.join(WORKFLOW_DIRECTORY,
                                         kwargs['workflow_id'])
        if not os.path.isdir(workflow_location):
            return 1, "Invalid workflow id, {0} is not a directory\n".format(workflow_location)
    try:
        if action == 'status':
            output = subprocess.check_output(['/usr/bin/pegasus-status',
                                              '-l',
                                              workflow_location],
                                             stderr=subprocess.STDOUT)
        elif action == 'remove':
            output = subprocess.check_output(['/usr/bin/pegasus-remove',
                                              workflow_location],
                                             stderr=subprocess.STDOUT)
        elif action == 'submit':
            if 'dax' not in kwargs:
                return 1, "DAX missing\n"
            elif 'conf' not in kwargs:
                return 1, "config file location missing\n"
            elif 'workflow_directory' not in kwargs:
                return 1, "workflow directory missing\n"

            output = subprocess.check_output(['/usr/bin/pegasus-plan',
                                              '--sites',
                                              'condorpool',
                                              '--dir',
                                              kwargs['workflow_directory'],
                                              '--conf',
                                              kwargs['conf'],
                                              '--output-dir',
                                              os.path.join(WORKFLOW_BASE_DIRECTORY,
                                                           'output'),
                                              '--dax',
                                              kwargs['dax'],
                                              '--submit'],
                                             stderr=subprocess.STDOUT)
        else:
            return 1, "Invalid pegasus action\n"
    except subprocess.CalledProcessError, err:
        return err.returncode, err.output

    return 0, output


def print_status(workflow_id, verbose=False):
    """
    Print the status of a workflow

    :param workflow_id:  pegasus id for workflow
    :param verbose:      whether to give full workflow details or a summary
    :return:             0 on success, 1 on error
    """
    if workflow_id is None:
        sys.stdout.write("Workflow id not given\n")
        return 0
    sys.stdout.write("Processing submission\n")
    exit_code, output = run_pegasus('status', workflow_id=workflow_id)
    if exit_code != 0:
        sys.stdout.write("An error occurred while getting job status\n")
        if output[1] is not None:
            sys.stdout.write("{0}\n".format(output))
    if verbose:
        sys.stdout.write("{0}\n".format(output))
        return exit_code
    job_summary = re.search(r'Summary: (\d+) Condor jobs(.*)', output)
    if job_summary is not None:
        idle_jobs = re.search(r'I:(\d+)', job_summary.group(2))
        num_idle = 0 
        if idle_jobs is not None:
            num_idle = idle_jobs.group(1)
        running_jobs = re.search(r'R:(\d+)', job_summary.group(2))
        num_running = 0 
        if running_jobs is not None:
            num_running = running_jobs.group(1)
        held_jobs = re.search(r'H:(\d+)', job_summary.group(2))
        num_held = 0 
        if held_jobs is not None:
            num_held = held_jobs.group(1)
        sys.stdout.write("Job information:\n")
        sys.stdout.write("{0} Idle jobs\n".format(num_idle))
        sys.stdout.write("{0} Running jobs\n".format(num_running))
        sys.stdout.write("{0} Held jobs\n".format(num_held))
        sys.stdout.write("{0} Total jobs\n".format(job_summary.group(1)))
        sys.stdout.write("\n")
    in_dag_status = False
    sys.stdout.write("Workflow information: \n")
    for line in cStringIO.StringIO(output).readlines():
        if 'UNRDY' in line or 'UNREADY' in line:
            sys.stdout.write(line + "\n")
            in_dag_status = True
            continue
        if in_dag_status:
            sys.stdout.write(line + "\n")
            if 'Summary' in line:
                break
    return exit_code


def gettime(workflow_id):
    """
    Get the submission time and date for a workflow
    :param workflow_id:  pegasus id for workflow
    :return:  a string with time and date of submission
    """
    year = workflow_id[0:4]
    month = workflow_id[4:6]
    day = workflow_id[6:8]
    hour = workflow_id[9:11]
    minute = workflow_id[11:13]
    return "{0}:{1} {2}-{3}-{4}".format(hour,
                                        minute,
                                        month,
                                        day,
                                        year)


def get_status(workflow_id):
    """
    Check and return the status of a given workflow

    :param workflow_id:  pegasus id for workflow
    :return: status of given workflow
    """
    exit_code, output = run_pegasus('status', workflow_id=workflow_id)
    if exit_code != 0:
        return 'UNKNOWN'
    in_dag_status = False
    for line in cStringIO.StringIO(output).readlines():
        if 'UNRDY' in line or 'UNREADY' in line:
            in_dag_status = True
            continue
        if in_dag_status:
            status_match = re.search('(?:\s+\d+){7}\s+\d+.\d+\s+(\w*)\s+',
                                     line)
            if status_match:
                return status_match.group(1)
    return 'UNKNOWN'


def list_workflows():
    """
    List the workflows currently in the system

    :return: 0 on success, 1 on error
    """
    try:
        check_and_create_workflow_dir()
        sys.stdout.write("Current workflows\n")
        sys.stdout.write("{0:10} {1:20} {2:20} {3:15} {4:10}\n".format('Subject',
                                                                       'Workflow',
                                                                       'Submit time',
                                                                       'Cores Used',
                                                                       'Status'))
        workflows = load_workflow_summary()
        if type(workflows) != dict:
            sys.stdout.write("Resetting saved workflow"
                             " information\n")
            workflows = regenerate_workflow_info()
        if workflows == {}:
            workflows = regenerate_workflow_info()
        for workflow in sorted(workflows, reverse=True):
            if len(workflows[workflow]) == 2:
                subject = workflows[workflow][0]
                cores = workflows[workflow][1]
                status = get_status(workflow)
                if status.upper() in ['COMPLETED', 'FAILED', 'ERROR']:
                    workflows[workflow].append(status)
            else:
                subject = workflows[workflow][0]
                cores = workflows[workflow][1]
                status = workflows[workflow][2]

            sys.stdout.write("{0:10} {1:20} {2:20} {3:<15} {4:10}\n".format(subject,
                                                                            workflow,
                                                                            gettime(workflow),
                                                                            cores,
                                                                            status))
        save_workflow_summary(workflows)
        return 0
    except IOError:
        return 1


def reset_workflows():
    """
    Reset the workflows currently stored

    :return: 0 on success, 1 on error
    """
    try:
        check_and_create_workflow_dir()
        workflows = regenerate_workflow_info()
        save_workflow_summary(workflows)
        return 0
    except IOError:
        return 1


def regenerate_workflow_info():
    """
    Scan workflow directories and regenerate the pickled info

    :return: workflow dictionary with information on workflows
    """
    workflows = {}
    for entry in os.listdir(WORKFLOW_DIRECTORY):
        info_filename = os.path.join(WORKFLOW_DIRECTORY, entry, 'fsurf_info')
        if not os.path.isfile(info_filename):
            continue
        with open(info_filename, 'r') as f:
            info = f.read(-1)
            try:
                subject_name, cores = info.split(' ')
                cores = int(cores.strip())
                workflows[entry] = [subject_name.strip(), cores]
            except ValueError:
                continue
    save_workflow_summary(workflows)
    return workflows


def remove_workflow(workflow_id):
    """
    Stop and remove a specified pegasus workflow

    :param workflow_id: pegasus id for workflow
    :return: 0 on success, 1 on error
    """
    workflow_location = os.path.join(WORKFLOW_DIRECTORY,
                                     workflow_id)
    sys.stdout.write("Removing workflow\n")
    exit_code, output = run_pegasus('remove', workflow_id=workflow_id)
    # job removed (code = 0) just now  or it's been removed earlier
    if exit_code == 0 or 'not found' in output:
        sys.stdout.write("Workflow {0} removed successfully\n".format(workflow_id))
        workflows = load_workflow_summary()
        if workflow_id in workflows:
            del workflows[workflow_id]
            save_workflow_summary(workflows)
        job_id = re.match(r'Job (\d+.\d+) marked for removal', output)
        if job_id is not None:
            sys.stdout.write("Waiting for running jobs to be removed...\n")
            count = 0
            while True:
                time.sleep(10)
                try:
                    output = subprocess.check_output(["/usr/bin/condor_q",  
                                                      job_id.group(1)])
                except subprocess.CalledProcessError:
                    sys.stdout.write("An error occurred while checking for "
                                     "running jobs, exiting...\n")
                    return 1
                if 'pegasus-dagman' not in output:
                    break
                count += 1
                if count > 30:
                    sys.stdout.write("Can't remove job, exiting...\n")
                    return 1
        sys.stdout.write("Jobs removed, removing workflow directory\n")
        try:
            shutil.rmtree(workflow_location)
        except shutil.Error:
            sys.stdout.write("Can't remove directory at "
                             "{0}, exiting...\n".format(workflow_location))
            exit_code = 1
    else:
        sys.stdout.write("Workflow {0} was not removed\n".format(workflow_id))
        sys.stdout.write("Following error occurred:\n")
        sys.stdout.write(output)
        sys.stdout.write("You may need to remove the workflow directory "
                         "at {0} manually\n".format(workflow_location))
    return exit_code


def save_workflow_info(workflow_id, subject_name, multicore):
    """
    Save workflow info to pickle file as well as saving a backup in case

    :param workflow_id: id for workflow
    :param subject_name: subject being processed
    :param multicore: True if job is using 8 cores, false otherwise
    :return: 0 on success, 1 on error
    """
    if multicore:
        cores = 8
    else:
        cores = 2
    exit_code = 0
    workflows = load_workflow_summary()
    workflows[workflow_id] = [subject_name, cores]
    save_workflow_summary(workflows)
    with open(os.path.join(WORKFLOW_DIRECTORY, workflow_id, "fsurf_info"), "w") as f:
        f.write("{0} {1}".format(subject_name, cores))
    return exit_code


def get_response(prompt):
    """
    Give a prompt to the user and return a True/False based on user input

    :param prompt: Text to present to user
    :return: True if user said yes, False otherwise
    """
    response = ""
    while response not in ['y', 'n']:
        response = raw_input(prompt)
        response = response.strip().lower()

    if response == 'y':
        return True
    else:
        return False


def submit_workflow(input_directory, subject_name, multicore=False,
                    workflow='diamond', deidentified=False, defaced=False):
    """
    Submit a workflow to OSG for processing

    :param input_directory:    path to file with MRI data in mgz format
    :param subject_name:  name of subject in the file
    :param multicore:     boolean indicating whether to use a multicore
                          workflow or not
    :param workflow:      string indicating type of workflow to run (serial,
                          diamond, single)
    :param deidentified:  boolean indicating whether MRI image has been
                          deidentified
    :param defaced:       boolean indicating whether MRI image has been defaced
    :return:              0 on success, 1 on error
    """
    if multicore:
        cores = 8
    else:
        cores = 2
    if subject_name is None:
        sys.stdout.write("Subject name is missing, exiting...\n")
        return 1
    if not deidentified:
        agree = get_response("Has the MRI data been deidentified "
                                "(This is required) [y/n]? ")
        if not agree:
            sys.stdout.write("MRI data must be deidentified, please remove "
                             "your MRI file immediately\n")
            return 1
    if not defaced:
        agree = get_response("Has the MRI data been defaced "
                                "(This is recommended) [y/n]? ")
        if not agree:
            sys.stdout.write("We recommend defacing MRI data\n")
            agree = get_response("Are you sure you want to submit this "
                                    "file [y/n]? ")
            if not agree:
                sys.stdout.write("Aborting submission on user request\n")
                return 1
    sys.stdout.write("Creating and submitting workflow\n")
    subject_file = os.path.join(input_directory,
                                "{0}_defaced.mgz".format(subject_name))
    subject_file = os.path.abspath(subject_file)
    if not os.path.isfile(subject_file):
        sys.stderr.write("{0} is not ".format(subject_file) +
                         "present and is needed, exiting\n")
        return 1
    dax = Pegasus.DAX3.ADAG('freesurfer')
    dax_subject_file = Pegasus.DAX3.File("{0}_defaced.mgz".format(subject_name))
    dax_subject_file.addPFN(Pegasus.DAX3.PFN("file://{0}".format(subject_file),
                                             "local"))
    dax.addFile(dax_subject_file)
    if workflow == 'serial':
        errors = fsurfer.create_serial_workflow(dax,
                                                cores,
                                                dax_subject_file,
                                                subject_name)
    elif workflow == 'diamond':
        errors = fsurfer.create_diamond_workflow(dax,
                                                 cores,
                                                 dax_subject_file,
                                                 subject_name)
    elif workflow == 'single':
        errors = fsurfer.create_single_workflow(dax,
                                                cores,
                                                dax_subject_file,
                                                subject_name)
    else:
        sys.stdout.write("Unknown workflow specified, using serial instead\n")
        errors = fsurfer.create_serial_workflow(dax,
                                                cores,
                                                dax_subject_file,
                                                subject_name)
    if not errors:
        dax.invoke('on_success', '/usr/bin/email_fsurf_notification.py --success')
        dax.invoke('on_error', '/usr/bin/email_fsurf_notification.py --failure')
        curr_date = time.strftime("%Y%m%d_%H%M%S", time.gmtime(time.time()))
        dax_name = "freesurfer_{0}.xml".format(curr_date)
        with open(dax_name, 'w') as f:
            dax.writeXML(f)
        exit_code, output = run_pegasus('submit',
                                        dax="{0}".format(dax_name),
                                        conf=PEGASUSRC_PATH,
                                        sites="condorpool",
                                        workflow_directory=WORKFLOW_BASE_DIRECTORY)
        if exit_code != 0:
            sys.stdout.write("An error occurred when generating and "
                             "submitting workflow, exiting...\n")
            sys.stdout.write("Error: \n")
            sys.stdout.write(output)
            os.unlink(dax_name)
            return 1
        capture_id = False
        for line in cStringIO.StringIO(output).readlines():
            if 'Your workflow has been started' in line:
                capture_id = True
            if capture_id and WORKFLOW_DIRECTORY in line:
                id_match = re.search(r'([T\d]+-\d+)'.format(WORKFLOW_DIRECTORY),
                                     line)
                if id_match is not None:
                    sys.stdout.write("Workflow submitted with an "
                                     "id of {0}\n".format(id_match.group(1)))
                    save_workflow_info(id_match.group(1),
                                       subject_name,
                                       multicore)
                else:
                    sys.stdout.write("Workflow submitted but could not get workflow id\n")
                break

        os.unlink(dax_name)
    return errors


def get_output(workflow_id):
    """
    Get output from a completed workflow and copy it to local directory

    :param workflow_id: pegasus id for workflow
    :return: 0 on success, 1 on error
    """
    if workflow_id is None:
        sys.stdout.write("No id given, exiting\n")
        return 1
    output_dir = os.path.join(WORKFLOW_OUTPUT_DIRECTORY, workflow_id)
    if not os.path.isdir(output_dir):
        sys.stdout.write("Output for workflow {0} does not exist\n".format(workflow_id))
        return 1
    sys.stdout.write("Getting output, this may take a little time.\n")
    output_files = glob.glob(os.path.join(WORKFLOW_OUTPUT_DIRECTORY,
                                          workflow_id,
                                          "*_output.tar.bz2"))
    if len(output_files) != 1:
        sys.stdout.write("Error getting output\n")
        return 1
    output_file = output_files[0]
    dest_file = os.path.basename(output_file)
    if os.path.exists(dest_file):
        sys.stdout.write("{0} exists, won't overwrite\n".format(dest_file))
        return 1
    shutil.copy(output_file, dest_file)
    sys.stdout.write("Output saved to {0}\n".format(dest_file))
    sys.stdout.write("To extract the results: tar xvjf {0}\n".format(dest_file))
    return 0


def get_log(workflow_id):
    """
    Get logs from a completed workflow and copy it to local directory

    :param workflow_id: pegasus id for workflow
    :return: 0 on success, 1 on error
    """
    if workflow_id is None:
        sys.stdout.write("No id given, exiting\n")
        return 1
    output_dir = os.path.join(WORKFLOW_OUTPUT_DIRECTORY, workflow_id)
    if not os.path.isdir(output_dir):
        sys.stdout.write("Output for workflow {0} does not exist\n".format(workflow_id))
        return 1
    sys.stdout.write("Getting output, this may take a little time.\n")
    output_file = os.path.join(WORKFLOW_OUTPUT_DIRECTORY,
                                workflow_id,
                                "recon-all.log")
    dest_file = os.path.basename(output_file)
    if os.path.exists(dest_file):
        sys.stdout.write("{0} exists, won't overwrite\n".format(dest_file))
        return 1
    shutil.copy(output_file, dest_file)
    sys.stdout.write("Output saved to {0}\n".format(dest_file))
    return 0


def save_workflow_summary(workflows):
    """
    Save workflow information to a file using pickle

    :param workflows: dictionary with information on workflows
    :return: None
    """
    pickle_filename = os.path.join(WORKFLOW_BASE_DIRECTORY, 'fsurf_workflows.pkl')
    if os.path.exists(pickle_filename) and not os.path.isfile(pickle_filename):
        sys.stdout.write("{0} is not a file, ".format(pickle_filename) +
                         "can't save workflow information\n")
        return
    with open(pickle_filename, 'wb') as f:
        cPickle.dump(workflows, f, cPickle.HIGHEST_PROTOCOL)


def load_workflow_summary():
    """
    Load workflow information from a file

    :return: dictionary with workflow information
    """
    pickle_filename = os.path.join(WORKFLOW_BASE_DIRECTORY, 'fsurf_workflows.pkl')
    if not os.path.exists(pickle_filename):
        return {}
    with open(pickle_filename, 'rb') as f:
        try:
            pickle = cPickle.load(f)
            return pickle
        except EOFError:
            return regenerate_workflow_info()


def check_groups():
    """
    Check groups that current user belongs to and exit with message if user
    isn't in freesurfer group

    :return: None
    """
    groups = os.getgroups()
    freesurfer_member = False
    for group in groups:
        if grp.getgrgid(group).gr_name == '@freesurfer':
            freesurfer_member = True
    if not freesurfer_member:
        sys.stderr.write("You need to be a member of the freesurfer "
                         "project to submit workflows.  Please request "
                         "membership using the OSG Connect website.\n")
        sys.exit(1)


def main():
    """
    Main function that parses arguments and generates the pegasus
    workflow

    :return: True if any errors occurred during DAX generaton
    """
    check_groups()
    parser = argparse.ArgumentParser(description="Process freesurfer information")
    # Arguments for action
    parser.add_argument('--version', action='version',
                        version='%(prog)s ' + VERSION)
    parser.add_argument('--submit', dest='action',
                        action='store_const', const='submit',
                        help='Submit job for processing')
    parser.add_argument('--list', dest='action',
                        action='store_const', const='list',
                        help='List current jobs')
    parser.add_argument('--status', dest='action',
                        action='store_const', const='status',
                        help='Get status of a specified job')
    parser.add_argument('--remove', dest='action',
                        action='store_const', const='remove',
                        help='Remove specified job')
    parser.add_argument('--output', dest='action',
                        action='store_const', const='output',
                        help='Get output from specified job')
    parser.add_argument('--reset', dest='action',
                        action='store_const', const='reset',
                        help=argparse.SUPPRESS)
    parser.add_argument('--log', dest='action',
                        action='store_const', const='log',
                        help="Get log file for specified job")
    # Arguments identifying workflow
    parser.add_argument('--id', dest='workflow_id',
                        action='store', help='id that specifies job')

    # Arguments for submit action
    parser.add_argument('--deidentified', dest="deidentified",
                        action="store_true", default=False,
                        help=argparse.SUPPRESS)
    parser.add_argument('--defaced', dest="defaced",
                        action="store_true", default=False,
                        help=argparse.SUPPRESS)
    # submit options for workflow type
    parser.add_argument('--diamond', dest="workflow_type",
                        action="store_const", const="diamond",
                        help=argparse.SUPPRESS)
    parser.add_argument('--serial', dest="workflow_type",
                        action="store_const", const="serial",
                        help=argparse.SUPPRESS)
    parser.add_argument('--single', dest="workflow_type",
                        action="store_const", const="single",
                        help=argparse.SUPPRESS)
    # submit options to identify subject and input
    parser.add_argument('--subject', dest='subject', default=None,
                        help='Subject id to process ')
    parser.add_argument('--dir', dest='input_directory',
                        default='.', help='directory containing input file')
    # multicore
    parser.add_argument('--dualcore', dest='multicore',
                        action='store_false', default=True,
                        help='Use 2 cores to process certain steps')
    # output verbose status messages
    parser.add_argument('--verbose', dest='verbose', default=False,
                        action='store_true',
                        help='Enable verbose output for status action')
    args = parser.parse_args(sys.argv[1:])

    if args.action == 'status':
        status = print_status(args.workflow_id, args.verbose)
    elif args.action == 'list':
        status = list_workflows()
    elif args.action == 'reset':
        status = reset_workflows()
    elif args.action == 'remove':
        status = remove_workflow(args.workflow_id)
    elif args.action == 'submit':
        if 'workflow_type' in args and args.workflow_type is not None:
            workflow = args.workflow_type
        else:
            workflow = 'diamond'
        status = submit_workflow(args.input_directory,
                                 args.subject,
                                 args.multicore,
                                 workflow,
                                 args.deidentified,
                                 args.defaced)
    elif args.action == 'output':
        status = get_output(args.workflow_id)
    elif args.action == 'log':
        status = get_log(args.workflow_id)
    else:
        sys.stdout.write("Must specify an action, exiting...\n")
        parser.print_help()
        status = 0
    sys.exit(status)

if __name__ == '__main__':
    # workaround missing subprocess.check_output
    if "check_output" not in dir(subprocess):  # duck punch it in!
        def check_output(*popenargs, **kwargs):
            """
            Run command with arguments and return its output as a byte string.

            Backported from Python 2.7 as it's implemented as pure python
            on stdlib.

            """
            process = subprocess.Popen(stdout=subprocess.PIPE, *popenargs, **kwargs)
            output, unused_err = process.communicate()
            retcode = process.poll()
            if retcode:
                cmd = kwargs.get("args")
                if cmd is None:
                    cmd = popenargs[0]
                error = subprocess.CalledProcessError(retcode, cmd)
                error.output = output
                raise error
            return output

        subprocess.check_output = check_output
    main()
